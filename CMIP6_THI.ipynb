{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CMIP6_THI.ipynb","private_outputs":true,"provenance":[{"file_id":"1OhneA9bsa1KqFwVrEJLnd0WhS5-gJXSR","timestamp":1599665001447},{"file_id":"1uXhMps3xa9CP1MEN7-kFmyMf_B5EtefT","timestamp":1599050059351}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Lk82YQGWUAes"},"source":["# CMIP6 Quantile"]},{"cell_type":"code","metadata":{"id":"SlTyVC-eGWpd"},"source":["%%capture\n","!pip install netcdf4 metpy colorcet\n","import os, fnmatch, cv2, gzip, shutil, datetime, numpy as np, pandas as pd, xarray as xr\n","import pylab as plt, plotly.express as px, seaborn as sns, colorcet as cc, altair as alt\n","from matplotlib.gridspec import GridSpec; from matplotlib.colors import from_levels_and_colors as flc\n","from metpy.units import units; import metpy.calc as mpcalc\n","\n","!pip install --upgrade zarr gcsfs cftime nc-time-axis\n","import fsspec, zarr, gcsfs; gcs = gcsfs.GCSFileSystem( token = 'anon' )\n","\n","%load_ext google.colab.data_table \n","from google.colab import drive; drive.mount( '/content/drive' )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iMJRHnWRn9Zd"},"source":["## Get list of models"]},{"cell_type":"code","metadata":{"id":"5iLSC1A7eB7e"},"source":["df = pd.read_csv( 'https://storage.googleapis.com/cmip6/cmip6-zarr-consolidated-stores.csv' )\n","df1 = df.query( \"activity_id=='ScenarioMIP' & variable_id == 'tas' & experiment_id == 'ssp585' & table_id == 'day'\" )\n","tas = df1.source_id.unique().tolist()\n","df1 = df.query( \"activity_id=='ScenarioMIP' & variable_id == 'huss' & experiment_id == 'ssp585' & table_id == 'day'\" )\n","huss = df1.source_id.unique().tolist()\n","ssp585_models = list( set(tas).intersection( set(huss) ) )\n","\n","df1 = df.query( \"activity_id=='ScenarioMIP' & variable_id == 'tas' & experiment_id == 'ssp126' & table_id == 'day'\" )\n","tas = df1.source_id.unique().tolist()\n","df1 = df.query( \"activity_id=='ScenarioMIP' & variable_id == 'huss' & experiment_id == 'ssp126' & table_id == 'day'\" )\n","huss = df1.source_id.unique().tolist()\n","ssp126_models = list( set(tas).intersection( set(huss) ) )\n","\n","df1 = df.query( \"activity_id=='ScenarioMIP' & variable_id == 'tas' & experiment_id == 'ssp245' & table_id == 'day'\" )\n","tas = df1.source_id.unique().tolist()\n","df1 = df.query( \"activity_id=='ScenarioMIP' & variable_id == 'huss' & experiment_id == 'ssp245' & table_id == 'day'\" )\n","huss = df1.source_id.unique().tolist()\n","ssp245_models = list( set(tas).intersection( set(huss) ) )\n","\n","df1 = df.query( \"activity_id=='ScenarioMIP' & variable_id == 'tas' & experiment_id == 'ssp370' & table_id == 'day'\" )\n","tas = df1.source_id.unique().tolist()\n","df1 = df.query( \"activity_id=='ScenarioMIP' & variable_id == 'huss' & experiment_id == 'ssp370' & table_id == 'day'\" )\n","huss = df1.source_id.unique().tolist()\n","ssp370_models = list( set(tas).intersection( set(huss) ) )\n","\n","df1 = df.query( \"activity_id=='CMIP' & variable_id == 'tas' & experiment_id == 'historical' & table_id == 'day'\" )\n","tas = df1.source_id.unique().tolist()\n","df1 = df.query( \"activity_id=='CMIP' & variable_id == 'huss' & experiment_id == 'historical' & table_id == 'day'\" )\n","huss = df1.source_id.unique().tolist()\n","hist_models = list( set(tas).intersection( set(huss) ) )\n","hist_models"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SmLa0veAugbv"},"source":["## loop through all models for historical\n","RH and dewpoint from the method of Bolton 1980 (http://www.eol.ucar.edu/projects/ceop/dm/documents/refdata_report/eqns.html)\n"]},{"cell_type":"code","metadata":{"id":"yLbywXf2q67m"},"source":["df = pd.read_csv( 'https://storage.googleapis.com/cmip6/cmip6-zarr-consolidated-stores.csv' )\n"," \n","#bad time 'SAM0-UNICON'\n","\n","for model in hist_models:\n","    print(model)\n","    df1 = df.query( f\"activity_id=='CMIP' & variable_id == 'huss' & experiment_id == 'historical' & table_id == 'day' & source_id == '{model}'\" )\n","    huss = xr.open_zarr( gcs.get_mapper( df1.zstore.values[0] ), consolidated = True )\n","    huss = huss.sel(time = slice('1985-01-01T12:00:00', '2014-12-30T12:00:00'))\n","\n","    df1 = df.query( f\"activity_id=='CMIP' & variable_id == 'tas' & experiment_id == 'historical' & table_id == 'day' & source_id == '{model}'\" )\n","    Ta = xr.open_zarr( gcs.get_mapper( df1.zstore.values[0] ), consolidated = True )\n","    Ta = Ta.sel(time = slice('1985-01-01T12:00:00', '2014-12-30T12:00:00')) - 273.15\n","\n","    RH = (huss.huss * 1013 / (0.378 * huss.huss + 0.622)) / (6.112 * np.exp((17.67 * Ta.tas)/(Ta.tas + 243.5)))\n","    RH = xr.where(RH < 1, RH, 1)\n","\n","    thi = (1.8 * Ta.tas + 32) - ( (0.55 - 0.55 * RH) * (1.8 * Ta.tas - 26) )\n","    thi = thi.chunk({'time': -1})\n","    thi = thi.groupby('time.dayofyear').median('time').to_dataset( name = 'THI' ).to_netcdf(f'/content/drive/My Drive/data/livestock/CMIP6/historical/{model}_hist_1985_2014_thi_doy_median.nc')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WZA5n0kTGtuI"},"source":["def run_ssp(ssp, model_list):\n","    for model in model_list:\n","        print(model)\n","        df1 = df.query( f\"activity_id=='ScenarioMIP' & variable_id == 'huss' & experiment_id == '{ssp}' & table_id == 'day' & source_id == '{model}'\" )\n","        huss = xr.open_zarr( gcs.get_mapper( df1.zstore.values[0] ), consolidated = True )\n","        huss = huss.sel(time = slice('2070-01-01T12:00:00', '2099-12-30T12:00:00'))\n","\n","        df1 = df.query( f\"activity_id=='ScenarioMIP' & variable_id == 'tas' & experiment_id == '{ssp}' & table_id == 'day' & source_id == '{model}'\" )\n","        Ta = xr.open_zarr( gcs.get_mapper( df1.zstore.values[0] ), consolidated = True )\n","        Ta = Ta.sel(time = slice('2070-01-01T12:00:00', '2099-12-30T12:00:00')) - 273.15\n","\n","        RH = (huss.huss * 1013 / (0.378 * huss.huss + 0.622)) / (6.112 * np.exp((17.67 * Ta.tas)/(Ta.tas + 243.5)))\n","        RH = xr.where(RH < 1, RH, 1)\n","\n","        thi = (1.8 * Ta.tas + 32) - ( (0.55 - 0.55 * RH) * (1.8 * Ta.tas - 26) )\n","        thi = thi.chunk({'time': -1})\n","        thi = thi.groupby('time.dayofyear').median('time').to_dataset( name = 'THI' ).to_netcdf(f'/content/drive/My Drive/data/livestock/CMIP6/{ssp}/{model}_{ssp}_2070_2099_thi_doy_median.nc')\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FDt_D7a57U_i"},"source":["run_ssp( 'ssp370', ssp370_models )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bIBufvRZ7DZG"},"source":["run_ssp( 'ssp126', ssp126_models )\n","run_ssp( 'ssp245', ssp245_models )\n","run_ssp( 'ssp585', ssp585_models )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZsOV7iAgab2l"},"source":["# Process model changes "]},{"cell_type":"code","metadata":{"id":"20HUEBnXW3S7"},"source":["histroot   = '/content/drive/My Drive/data/livestock/CMIP6/historical/'\n","ssp126root = '/content/drive/My Drive/data/livestock/CMIP6/ssp126/'\n","ssp585root = '/content/drive/My Drive/data/livestock/CMIP6/ssp585/'\n","\n","hist   = os.listdir( histroot   )\n","ssp126 = os.listdir( ssp126root )\n","ssp585 = os.listdir( ssp585root )\n","\n","ssp126 = fnmatch.filter( ssp126, '*doy*' )\n","for file in ssp126:\n","    model = file.split('_')[0]\n","    print( model )\n","    h    = xr.open_dataarray( histroot   + fnmatch.filter( hist,   f'{model}*doy*' )[0] )\n","    ssp1 = xr.open_dataarray( ssp126root + fnmatch.filter( ssp126, f'{model}*doy*' )[0] ) - h\n","    ssp5 = xr.open_dataarray( ssp585root + fnmatch.filter( ssp585, f'{model}*doy*' )[0] ) - h \n","    ssp1.to_netcdf( f'/content/drive/My Drive/data/livestock/CMIP6/{model}_ssp126_change_doy_median.nc4' )\n","    ssp5.to_netcdf( f'/content/drive/My Drive/data/livestock/CMIP6/{model}_ssp585_change_doy_median.nc4' )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-otTg40SanjL"},"source":["# resize CMIP6 and map change onto ERA baseline"]},{"cell_type":"code","metadata":{"id":"XL15OAlrY9P_"},"source":["era = xr.open_dataarray('/content/drive/My Drive/data/livestock/ERA5/era_doy_media.nc4')\n","lon = era.lon.values\n","lat = era.lat.values\n","doy = era.dayofyear.values\n","pro_thresh = 66.5    # production threshold\n","era = era.values\n","era_thresh = np.ma.masked_where( era< pro_thresh, era )\n","era_pro = np.ma.count( era_thresh, axis = 0 )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k-lzuklCY9q0"},"source":["root = '/content/drive/My Drive/data/livestock/CMIP6/'\n","filelist = [root + i for i in fnmatch.filter( os.listdir(root), '*ssp*change*.nc4')]\n","\n","for file in filelist:\n","    thiout = np.ma.zeros( ( 366, lat.shape[0], lon.shape[0] ) )\n","    out    = np.ma.zeros( ( 4, lat.shape[0], lon.shape[0] ) )\n","    model  = file.split( '/' )[-1].split( '_chang' )[0]\n","    print(model)\n","    thi = xr.open_dataarray( file )\n","    days = thi.dayofyear.values\n","    thi = thi.values\n","\n","    if days.shape[0] == 366:\n","        for i in range(366): thiout[i,:,:] = np.flip(cv2.resize(thi[i,:,:], (1440, 721) ), axis = 0)\n","\n","    elif days.shape[0] == 365:\n","        for i in range(60): thiout[i,:,:] = np.flip(cv2.resize(thi[i,:,:], (1440, 721) ), axis = 0)\n","        thiout[60,:,:] = np.flip(cv2.resize(thi[59,:,:], (1440, 721) ), axis = 0)\n","        for i in range(61,365): thiout[i,:,:] = np.flip(cv2.resize(thi[i-1,:,:], (1440, 721) ), axis = 0)\n","\n","    elif days.shape[0] == 360:\n","        for i in range(0,30): thiout[i,:,:] = np.flip(cv2.resize(thi[i,:,:], (1440, 721) ), axis = 0)\n","        thiout[30,:,:] = np.flip(cv2.resize(thi[29,:,:], (1440, 721) ), axis = 0)\n","        for i in range(31,153): thiout[i,:,:] = np.flip(cv2.resize(thi[i-1,:,:], (1440, 721) ), axis = 0)\n","        thiout[153,:,:] = np.flip(cv2.resize(thi[152,:,:], (1440, 721) ), axis = 0)\n","        for i in range(154,214): thiout[i,:,:] = np.flip(cv2.resize(thi[i-3,:,:], (1440, 721) ), axis = 0)\n","        thiout[214,:,:] = np.flip(cv2.resize(thi[210,:,:], (1440, 721) ), axis = 0)\n","        for i in range(215,245): thiout[i,:,:] = np.flip(cv2.resize(thi[i-4,:,:], (1440, 721) ), axis = 0)\n","        thiout[245,:,:] = np.flip(cv2.resize(thi[240,:,:], (1440, 721) ), axis = 0)\n","        for i in range(246,306): thiout[i,:,:] = np.flip(cv2.resize(thi[i-5,:,:], (1440, 721) ), axis = 0)\n","        thiout[306,:,:] = np.flip(cv2.resize(thi[300,:,:], (1440, 721) ), axis = 0)    \n","        for i in range(306,365): thiout[i,:,:] = np.flip(cv2.resize(thi[i-6,:,:], (1440, 721) ), axis = 0)\n","        thiout[365,:,:] = np.flip(cv2.resize(thi[359,:,:], (1440, 721) ), axis = 0) \n","    \n","    del thi    \n","    thiout = thiout + era\n","\n","    ds = xr.Dataset( coords={'lat': lat, 'lon':  lon} )\n","    \n","    temp = np.ma.masked_where( thiout < pro_thresh, thiout ).count( axis = 0 )\n","    ds['pro_days'] = (['lat', 'lon'],  temp)\n","    temp = np.sum( np.ma.masked_where( thiout < pro_thresh, thiout ) - pro_thresh, axis = 0 )\n","    ds['pro_HL'] = (['lat', 'lon'],  temp)\n","\n","    ds.to_netcdf(f'/content/drive/My Drive/data/livestock/CMIP6/{model}_665thresh.nc4')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s-vUOCiKGukW"},"source":["# OLD\n","\n","---\n","\n","\n","\n","---\n","\n","\n","\n","---\n","\n"]},{"cell_type":"code","metadata":{"id":"7m57nuQuiMJv"},"source":["df = pd.read_csv( 'https://storage.googleapis.com/cmip6/cmip6-zarr-consolidated-stores.csv' )\n"," \n","#bad time 'SAM0-UNICON'\n","#no tas: 'FGOALS-f3-L'\n","# bad lat or lon ['EC-Earth3', 'EC-Earth3-Veg', 'CESM2-WACCM']\n","# too large 'CNRM-CM6-1-HR'\n","\n","hist_models = ['NorESM2-LM','MPI-ESM-1-2-HAM','ACCESS-ESM1-5','MPI-ESM1-2-HR','MPI-ESM1-2-LR','BCC-CSM2-MR','KACE-1-0-G','UKESM1-0-LL','HadGEM3-GC31-LL']\n","\n","for model in hist_models:\n","    print(model)\n","    df1 = df.query( f\"activity_id=='CMIP' & variable_id == 'huss' & experiment_id == 'historical' & table_id == 'day' & source_id == '{model}'\" )\n","    huss = xr.open_zarr( gcs.get_mapper( df1.zstore.values[0] ), consolidated = True )\n","    huss = huss.sel(time = slice('1985-01-01T12:00:00', '2014-12-30T12:00:00'))\n","\n","    df1 = df.query( f\"activity_id=='CMIP' & variable_id == 'tasmax' & experiment_id == 'historical' & table_id == 'day' & source_id == '{model}'\" )\n","    Ta = xr.open_zarr( gcs.get_mapper( df1.zstore.values[0] ), consolidated = True )\n","    Ta = Ta.sel(time = slice('1985-01-01T12:00:00', '2014-12-30T12:00:00')) - 273.15\n","\n","    RH = (huss.huss * 1013 / (0.378 * huss.huss + 0.622)) / (6.112 * np.exp((17.67 * Ta.tasmax)/(Ta.tasmax + 243.5)))\n","    RH = xr.where(RH < 1, RH, 1)\n","    Tdp = 243.5 * (np.log(RH) + ( ( 17.67 * Ta.tasmax ) / ( 243.5 + Ta.tasmax ) ) ) / ( 17.67 - np.log(RH) - ( ( 17.67 * Ta.tasmax ) / ( 243.5 + Ta.tasmax ) ) )\n","\n","    thi = Ta.tasmax + 0.36*Tdp + 41.2\n","    thi = thi.chunk({'time': -1})\n","    thiq = thi.quantile( np.arange(0.01,1, 0.01), dim = 'time' )\n","    thiq.to_dataset( name = 'THI_quants' ).to_netcdf(f'/content/drive/My Drive/data/livestock/CMIP6/historical/kibler/max/{model}_hist_1985_2014_thi_quantiles.nc')\n","    thi.groupby('time.dayofyear').median('time').to_dataset( name = 'THI' ).to_netcdf(f'/content/drive/My Drive/data/livestock/CMIP6/historical/kibler/max/{model}_hist_1985_2014_thi_doy_median.nc')\n","\n","    thi = 0.80*Ta.tasmax + Ta.tasmax*RH - 1.4*RH + 46.4\n","    thi = thi.chunk({'time': -1})\n","    thiq = thi.quantile( np.arange(0.01,1, 0.01), dim = 'time' )\n","    thiq.to_dataset( name = 'THI_quants' ).to_netcdf(f'/content/drive/My Drive/data/livestock/CMIP6/historical/thom/max/{model}_hist_1985_2014_thi_quantiles.nc')\n","    thi.groupby('time.dayofyear').median('time').to_dataset( name = 'THI' ).to_netcdf(f'/content/drive/My Drive/data/livestock/CMIP6/historical/thom/max/{model}_hist_1985_2014_thi_doy_median.nc')\n","\n","    df1 = df.query( f\"activity_id=='CMIP' & variable_id == 'tas' & experiment_id == 'historical' & table_id == 'day' & source_id == '{model}'\" )\n","    Ta = xr.open_zarr( gcs.get_mapper( df1.zstore.values[0] ), consolidated = True )\n","    Ta = Ta.sel(time = slice('1985-01-01T12:00:00', '2014-12-30T12:00:00')) - 273.15\n","\n","    RH = (huss.huss * 1013 / (0.378 * huss.huss + 0.622)) / (6.112 * np.exp((17.67 * Ta.tas)/(Ta.tas + 243.5)))\n","    RH = xr.where(RH < 1, RH, 1)\n","    Tdp = 243.5 * (np.log(RH) + ( ( 17.67 * Ta.tas ) / ( 243.5 + Ta.tas ) ) ) / ( 17.67 - np.log(RH) - ( ( 17.67 * Ta.tas ) / ( 243.5 + Ta.tas ) ) )\n","\n","    thi = Ta.tas + 0.36*Tdp + 41.2\n","    thi = thi.chunk({'time': -1})\n","    thiq = thi.quantile( np.arange(0.01,1, 0.01), dim = 'time' )\n","    thiq.to_dataset( name = 'THI_quants' ).to_netcdf(f'/content/drive/My Drive/data/livestock/CMIP6/historical/kibler/mean/{model}_hist_1985_2014_thi_quantiles.nc')\n","    thi.groupby('time.dayofyear').median('time').to_dataset( name = 'THI' ).to_netcdf(f'/content/drive/My Drive/data/livestock/CMIP6/historical/kibler/mean/{model}_hist_1985_2014_thi_doy_median.nc')\n","\n","    thi = 0.80*Ta.tas + Ta.tas*RH - 1.4*RH + 46.4\n","    thi = thi.chunk({'time': -1})\n","    thiq = thi.quantile( np.arange(0.01,1, 0.01), dim = 'time' )\n","    thiq.to_dataset( name = 'THI_quants' ).to_netcdf(f'/content/drive/My Drive/data/livestock/CMIP6/historical/thom/mean/{model}_hist_1985_2014_thi_quantiles.nc')\n","    thi.groupby('time.dayofyear').median('time').to_dataset( name = 'THI' ).to_netcdf(f'/content/drive/My Drive/data/livestock/CMIP6/historical/thom/mean/{model}_hist_1985_2014_thi_doy_median.nc')\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EaRp7AvSQW0U"},"source":["df = pd.read_csv( 'https://storage.googleapis.com/cmip6/cmip6-zarr-consolidated-stores.csv' )\n"," \n","#bad time 'SAM0-UNICON'\n","#no tas: 'FGOALS-f3-L'\n","# bad lat or lon ['EC-Earth3', 'EC-Earth3-Veg', 'CESM2-WACCM']\n","# too large 'CNRM-CM6-1-HR'\n","\n","hist_models = ['NorESM2-LM','MPI-ESM-1-2-HAM','ACCESS-ESM1-5','MPI-ESM1-2-HR','MPI-ESM1-2-LR','BCC-CSM2-MR','KACE-1-0-G','UKESM1-0-LL','HadGEM3-GC31-LL']\n","\n","for model in hist_models:\n","    print(model)\n","    df1 = df.query( f\"activity_id=='CMIP' & variable_id == 'huss' & experiment_id == 'historical' & table_id == 'day' & source_id == '{model}'\" )\n","    huss = xr.open_zarr( gcs.get_mapper( df1.zstore.values[0] ), consolidated = True )\n","    huss = huss.sel(time = slice('1985-01-01T12:00:00', '2014-12-30T12:00:00'))\n","\n","\n","    df1 = df.query( f\"activity_id=='CMIP' & variable_id == 'tas' & experiment_id == 'historical' & table_id == 'day' & source_id == '{model}'\" )\n","    Ta = xr.open_zarr( gcs.get_mapper( df1.zstore.values[0] ), consolidated = True )\n","    Ta = Ta.sel(time = slice('1985-01-01T12:00:00', '2014-12-30T12:00:00')) - 273.15\n","\n","    RH = (huss.huss * 1013 / (0.378 * huss.huss + 0.622)) / (6.112 * np.exp((17.67 * Ta.tas)/(Ta.tas + 243.5)))\n","    RH = xr.where(RH < 1, RH, 1)\n","    Tdp = 243.5 * (np.log(RH) + ( ( 17.67 * Ta.tas ) / ( 243.5 + Ta.tas ) ) ) / ( 17.67 - np.log(RH) - ( ( 17.67 * Ta.tas ) / ( 243.5 + Ta.tas ) ) )\n","\n","    thi = Ta.tas + 0.36*Tdp + 41.2\n","    thi = thi.chunk({'time': -1})\n","    thiq = thi.quantile( np.arange(0.01,1, 0.01), dim = 'time' )\n","    thiq.to_dataset( name = 'THI_quants' ).to_netcdf(f'/content/drive/My Drive/data/livestock/CMIP6/historical/kibler/mean/{model}_hist_1985_2014_thi_quantiles.nc')\n","    thi.groupby('time.dayofyear').median('time').to_dataset( name = 'THI' ).to_netcdf(f'/content/drive/My Drive/data/livestock/CMIP6/historical/kibler/mean/{model}_hist_1985_2014_thi_doy_median.nc')\n","\n","    thi = 0.80*Ta.tas + Ta.tas*RH - 1.4*RH + 46.4\n","    thi = thi.chunk({'time': -1})\n","    thiq = thi.quantile( np.arange(0.01,1, 0.01), dim = 'time' )\n","    thiq.to_dataset( name = 'THI_quants' ).to_netcdf(f'/content/drive/My Drive/data/livestock/CMIP6/historical/thom/mean/{model}_hist_1985_2014_thi_quantiles.nc')\n","    thi.groupby('time.dayofyear').median('time').to_dataset( name = 'THI' ).to_netcdf(f'/content/drive/My Drive/data/livestock/CMIP6/historical/thom/mean/{model}_hist_1985_2014_thi_doy_median.nc')\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8bSUk30pN9sx"},"source":["def run_ssp(ssp, model_list):\n","    for model in model_list:\n","        print(model)\n","        df1 = df.query( f\"activity_id=='ScenarioMIP' & variable_id == 'huss' & experiment_id == '{ssp}' & table_id == 'day' & source_id == '{model}'\" )\n","        huss = xr.open_zarr( gcs.get_mapper( df1.zstore.values[0] ), consolidated = True )\n","        huss = huss.sel(time = slice('2070-01-01T12:00:00', '2099-12-30T12:00:00'))\n","\n","        df1 = df.query( f\"activity_id=='ScenarioMIP' & variable_id == 'tasmax' & experiment_id == '{ssp}' & table_id == 'day' & source_id == '{model}'\" )\n","        Ta = xr.open_zarr( gcs.get_mapper( df1.zstore.values[0] ), consolidated = True )\n","        Ta = Ta.sel(time = slice('2070-01-01T12:00:00', '2099-12-30T12:00:00')) - 273.15\n","\n","        RH = (huss.huss * 1013 / (0.378 * huss.huss + 0.622)) / (6.112 * np.exp((17.67 * Ta.tasmax)/(Ta.tasmax + 243.5)))\n","        RH = xr.where(RH < 1, RH, 1)\n","        Tdp = 243.5 * (np.log(RH) + ( ( 17.67 * Ta.tasmax ) / ( 243.5 + Ta.tasmax ) ) ) / ( 17.67 - np.log(RH) - ( ( 17.67 * Ta.tasmax ) / ( 243.5 + Ta.tasmax ) ) )\n","\n","        thi = Ta.tasmax + 0.36*Tdp + 41.2\n","        thi = thi.chunk({'time': -1})\n","        thiq = thi.quantile( np.arange(0.01,1, 0.01), dim = 'time' )\n","        thiq.to_dataset(name='THI_quants').to_netcdf(f'/content/drive/My Drive/data/livestock/CMIP6/{ssp}/kibler/max/{model}_{ssp}_2070_2099_thi_quantiles.nc')\n","        thi.groupby('time.dayofyear').median('time').to_dataset( name = 'THI' ).to_netcdf(f'/content/drive/My Drive/data/livestock/CMIP6/{ssp}/kibler/max/{model}_{ssp}_2070_2099_thi_doy_median.nc')\n","\n","        thi = 0.80*Ta.tasmax + Ta.tasmax*RH - 1.4*RH + 46.4\n","        thi = thi.chunk({'time': -1})\n","        thiq = thi.quantile( np.arange(0.01,1, 0.01), dim = 'time' )\n","        thiq.to_dataset(name='THI_quants').to_netcdf(f'/content/drive/My Drive/data/livestock/CMIP6/{ssp}/thom/max/{model}_{ssp}_2070_2099_thi_quantiles.nc')\n","        thi.groupby('time.dayofyear').median('time').to_dataset( name = 'THI' ).to_netcdf(f'/content/drive/My Drive/data/livestock/CMIP6/{ssp}/thom/max/{model}_{ssp}_2070_2099_thi_doy_median.nc')\n","\n","        df1 = df.query( f\"activity_id=='ScenarioMIP' & variable_id == 'tas' & experiment_id == '{ssp}' & table_id == 'day' & source_id == '{model}'\" )\n","        Ta = xr.open_zarr( gcs.get_mapper( df1.zstore.values[0] ), consolidated = True )\n","        Ta = Ta.sel(time = slice('2070-01-01T12:00:00', '2099-12-30T12:00:00')) - 273.15\n","\n","        RH = (huss.huss * 1013 / (0.378 * huss.huss + 0.622)) / (6.112 * np.exp((17.67 * Ta.tas)/(Ta.tas + 243.5)))\n","        RH = xr.where(RH < 1, RH, 1)\n","        Tdp = 243.5 * (np.log(RH) + ( ( 17.67 * Ta.tas ) / ( 243.5 + Ta.tas ) ) ) / ( 17.67 - np.log(RH) - ( ( 17.67 * Ta.tas ) / ( 243.5 + Ta.tas ) ) )\n","\n","        thi = Ta.tas + 0.36*Tdp + 41.2\n","        thi = thi.chunk({'time': -1})\n","        thiq = thi.quantile( np.arange(0.01,1, 0.01), dim = 'time' )\n","        thiq.to_dataset(name = 'THI_quants').to_netcdf(f'/content/drive/My Drive/data/livestock/CMIP6/{ssp}/kibler/mean/{model}_{ssp}_2070_2099_thi_quantiles.nc')\n","        thi.groupby('time.dayofyear').median('time').to_dataset( name = 'THI' ).to_netcdf(f'/content/drive/My Drive/data/livestock/CMIP6/{ssp}/kibler/mean/{model}_{ssp}_2070_2099_thi_doy_median.nc')\n","\n","        thi = 0.80*Ta.tas + Ta.tas*RH - 1.4*RH + 46.4\n","        thi = thi.chunk({'time': -1})\n","        thiq = thi.quantile( np.arange(0.01,1, 0.01), dim = 'time' )\n","        thiq.to_dataset(name = 'THI_quants').to_netcdf(f'/content/drive/My Drive/data/livestock/CMIP6/{ssp}/thom/mean/{model}_{ssp}_2070_2099_thi_quantiles.nc')\n","        thi.groupby('time.dayofyear').median('time').to_dataset( name = 'THI' ).to_netcdf(f'/content/drive/My Drive/data/livestock/CMIP6/{ssp}/thom/mean/{model}_{ssp}_2070_2099_thi_doy_median.nc')\n","\n","#models = ['CanESM5','BCC-CSM2-MR']\n","\n","#run_ssp( 'ssp585', models )\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_7LbBMDBQmj5"},"source":["df = pd.read_csv( 'https://storage.googleapis.com/cmip6/cmip6-zarr-consolidated-stores.csv' )\n","\n","def run_ssp(ssp, model_list):\n","    for model in model_list:\n","        print(model)\n","        df1 = df.query( f\"activity_id=='ScenarioMIP' & variable_id == 'huss' & experiment_id == '{ssp}' & table_id == 'day' & source_id == '{model}'\" )\n","        huss = xr.open_zarr( gcs.get_mapper( df1.zstore.values[0] ), consolidated = True )\n","        huss = huss.sel(time = slice('2070-01-01T12:00:00', '2099-12-30T12:00:00'))\n","\n","        df1 = df.query( f\"activity_id=='ScenarioMIP' & variable_id == 'tas' & experiment_id == '{ssp}' & table_id == 'day' & source_id == '{model}'\" )\n","        Ta = xr.open_zarr( gcs.get_mapper( df1.zstore.values[0] ), consolidated = True )\n","        Ta = Ta.sel(time = slice('2070-01-01T12:00:00', '2099-12-30T12:00:00')) - 273.15\n","\n","        RH = (huss.huss * 1013 / (0.378 * huss.huss + 0.622)) / (6.112 * np.exp((17.67 * Ta.tas)/(Ta.tas + 243.5)))\n","        RH = xr.where(RH < 1, RH, 1)\n","\n","        thi = 0.80*Ta.tas + Ta.tas*RH - 1.4*RH + 46.4\n","        thi = thi.chunk({'time': -1})\n","        thiq = thi.quantile( np.arange(0.01,1, 0.01), dim = 'time' )\n","        thiq.to_dataset(name = 'THI_quants').to_netcdf(f'/content/drive/My Drive/data/livestock/CMIP6/{ssp}/thom/mean/{model}_{ssp}_2070_2099_thi_quantiles.nc')\n","        thi.groupby('time.dayofyear').median('time').to_dataset( name = 'THI' ).to_netcdf(f'/content/drive/My Drive/data/livestock/CMIP6/{ssp}/thom/mean/{model}_{ssp}_2070_2099_thi_doy_median.nc')\n","\n","run_ssp( 'ssp126',  ['BCC-CSM2-MR','CanESM5','MPI-ESM1-2-HR','KACE-1-0-G','UKESM1-0-LL','GFDL-ESM4','CNRM-CM6-1','CNRM-ESM2-1','MRI-ESM2-0'] )\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vrBIoGr1dRzC"},"source":["df = pd.read_csv( 'https://storage.googleapis.com/cmip6/cmip6-zarr-consolidated-stores.csv' )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wsiKTF1s0OxK"},"source":["models = ['MPI-ESM1-2-HR',\n"," 'CNRM-CM6-1-HR',\n"," 'MRI-ESM2-0',\n"," 'CNRM-CM6-1',\n"," 'UKESM1-0-LL',\n"," 'CanESM5',\n"," 'KACE-1-0-G',\n"," 'BCC-CSM2-MR',\n"," 'GFDL-ESM4',\n"," 'CNRM-ESM2-1']\n","\n","run_ssp('ssp126', models)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ENDfWVqSVAYK"},"source":["\n","\n","---\n","\n","# OLD\n","\n","---\n","\n"]},{"cell_type":"code","metadata":{"id":"dv9kBbW6MvXO"},"source":["years = np.arange(1985,2015,1)\n","months = ['01','02','03','04','05','06','07','08','09','10','11','12']\n","\n","for year in years:\n","    for month in months:\n","        print(year, month)\n","        client.download_file( era5_bucket, f'{year}/{month}/data/air_temperature_at_2_metres.nc', \n","                            f'/content/{year}_{month}_air_temperature_at_2_metres.nc' )\n","\n","        client.download_file( era5_bucket, f'{year}/{month}/data/dew_point_temperature_at_2_metres.nc', \n","                            f'/content/{year}_{month}_dew_point_temperature_at_2_metres.nc' )\n","\n","        Ta = xr.open_dataarray(f'/content/{year}_{month}_air_temperature_at_2_metres.nc') - 273.15\n","        Tdp = xr.open_dataarray(f'/content/{year}_{month}_dew_point_temperature_at_2_metres.nc') - 273.15\n","        RH  = np.exp( ( 17.625 * Tdp ) / ( 243.04 + Tdp ) ) / np.exp( ( 17.625 * Ta ) / ( 243.04 + Ta ) )\n","\n","        thi = Ta + 0.36*Tdp + 41.2\n","        thim = thi.resample( time0 = '1D' ).max()\n","        thim.to_dataset(name='THI').to_netcdf(f'/content/drive/My Drive/data/livestock/ERA5/kibler/max/{year}_{month}_daily_max_thi.nc')\n","        thim = thi.resample( time0 = '1D' ).mean()\n","        thim.to_dataset(name='THI').to_netcdf(f'/content/drive/My Drive/data/livestock/ERA5/kibler/mean/{year}_{month}_daily_mean_thi.nc')\n","        \n","        del thi \n","        \n","        RH  = np.exp( ( 17.625 * Tdp ) / ( 243.04 + Tdp ) ) / np.exp( ( 17.625 * Ta ) / ( 243.04 + Ta ) )\n","        del Tdp\n","\n","        thi = 0.80*Ta + Ta*RH - 1.4*RH + 46.4\n","        thim = thi.resample( time0 = '1D' ).max()\n","        thim.to_dataset(name='THI').to_netcdf(f'/content/drive/My Drive/data/livestock/ERA5/thom/max/{year}_{month}_daily_max_thi.nc')\n","\n","        thim = thi.resample( time0 = '1D' ).mean()\n","        thim.to_dataset(name='THI').to_netcdf(f'/content/drive/My Drive/data/livestock/ERA5/thom/mean/{year}_{month}_daily_mean_thi.nc')\n","\n","        del thi\n","\n","        os.remove(f'/content/{year}_{month}_air_temperature_at_2_metres.nc')\n","        os.remove(f'/content/{year}_{month}_dew_point_temperature_at_2_metres.nc')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v8mJAmmkcVQQ"},"source":["keys = []\n","date = datetime.date( 1985, 1, 1 ) # update to desired date\n","prefix = date.strftime( '%Y/%m/' )\n","\n","response = client.list_objects_v2( Bucket=era5_bucket, Prefix=prefix )\n","response_meta = response.get( 'ResponseMetadata' )\n","\n","if response_meta.get('HTTPStatusCode') == 200:\n","    contents = response.get('Contents')\n","    if contents == None: print(\"No objects are available for %s\" % date.strftime('%B, %Y'))\n","    else:\n","        for obj in contents: keys.append(obj.get('Key'))\n","        print( \"There are %s objects available for %s\\n--\" % (len(keys), date.strftime('%B, %Y')))\n","        for k in keys: print(k)\n","else: print( \"There was an error with your request.\" )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aJvTTpIqMzM8"},"source":["ds = xr.open_dataarray(f'/content/{year}_{month}_air_temperature_at_2_metres.nc')\n","ds.metpy.convert_units('degC')\n","\n","dsd = xr.open_dataarray(f'/content/{year}_{month}_dew_point_temperature_at_2_metres.nc')\n","dsd.metpy.convert_units('degC')\n","\n","rh = mpcalc.relative_humidity_from_dewpoint( ds , dsd ) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ho5MRcnJSFwc"},"source":["### Reference ###"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s6B3jV7MGzIW"},"source":["def THI(spec, Ta, Tdp = False, huss = False):\n","    # takes dry-bulb temperature in C and either dewpoint (C) or specific humidity (kg/kg)\n","    # returns THI based on whichever specificciation you choose\n","    Ta = np.nan_to_num( np.atleast_1d( Ta ) )\n","    temp  = Ta * units.degC\n","    press = np.array( 1e5 ) * units.hectopascal\n","\n","    if huss == False: \n","        Tdp = np.nan_to_num( np.atleast_1d( Tdp ) )\n","        dew = Tdp * units.degC \n","        RH  = np.asarray( mpcalc.relative_humidity_from_dewpoint( temp , dew ) )\n","        ### Wet bulb from Stull, 2011 Journal of Applied Meterology and Climatology, Equation 1\n","        #Twb = Ta * np.arctan( 0.151977 * (RH + 8.313659)**(1/2) ) + np.arctan(Ta+ RH) - np.arctan(RH - 1.676331) + 0.00391838 * (RH)**(3/2) * np.arctan(0.023101 * RH) - 4.686035\n","\n","    elif Tdp == False:\n","        huss = np.nan_to_num( np.atleast_1d( huss ) )\n","        SH  =  huss * units.dimensionless\n","        RH  = np.asarray( mpcalc.relative_humidity_from_specific_humidity(SH, temp, press ) )\n","        Tdp = np.asarray( mpcalc.dewpoint_from_specific_humidity( SH, temp, press ) )\n","\n","    if   spec == 'thom':   thi = 0.80*Ta + Ta*RH - 1.4*RH + 46.4      ## adapted from Thom59, they do something goofy with the conversion\n","    elif spec == 'kibler': thi = 1.00*Ta + 0.36*Tdp + 41.2            ## Kibler 64, NRC71, then yousef85\n","\n","    thi = np.ma.masked_outside( np.nan_to_num( thi ), 1, 200 ).compressed()\n","\n","    return thi"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x-yEU7m9H87f"},"source":["ds.temperature.metpy.convert_units('degC')\n","ds.\n","\n","thiT = 0.80*Ta + Ta*RH - 1.4*RH + 46.4      \n","thiK = 1.00*Ta + 0.36*Tdp + 41.2            \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rZCWc3j1yFbs"},"source":["!pip install --upgrade botocore s3fs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SLCQfarcxyRX"},"source":["import datetime\n","from dask.distributed import LocalCluster, Client\n","import s3fs\n","import boto3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xfNxrlqhy0yG"},"source":["import botocore"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CuaUOWlDzGPI"},"source":["era5_bucket = 'era5-pds'\n","client = boto3.client('s3', config=botocore.client.Config(signature_version=botocore.UNSIGNED))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sOk8LW46ysb4"},"source":["keys = []\n","date = datetime.date( 2019, 1, 1 ) # update to desired date\n","prefix = 'zarr/2008/01/data/'  \n","\n","response = client.list_objects_v2( Bucket=era5_bucket, Prefix=prefix )\n","response_meta = response.get( 'ResponseMetadata' )\n","\n","if response_meta.get('HTTPStatusCode') == 200:\n","    contents = response.get('Contents')\n","    if contents == None: print(\"No objects are available for %s\" % date.strftime('%B, %Y'))\n","    else:\n","        for obj in contents: keys.append(obj.get('Key'))\n","        print( \"There are %s objects available for %s\\n--\" % (len(keys), date.strftime('%B, %Y')))\n","        for k in keys: print(k)\n","else: print( \"There was an error with your request.\" )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q5eiY7kpzYTL"},"source":["fs = s3fs.S3FileSystem(anon=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V7_vKim2zbqf"},"source":["def inc_mon(indate):\n","    if indate.month < 12:\n","        return datetime.datetime(indate.year, indate.month+1, 1)\n","    else:\n","        return datetime.datetime(indate.year+1, 1, 1)\n","\n","def gen_d_range(start, end):\n","    rr = []\n","    while start <= end:\n","        rr.append(start)\n","        start = inc_mon(start)\n","    return rr\n","\n","def get_z(dtime,var):\n","    f_zarr = 'era5-pds/zarr/{year}/{month:02d}/data/{var}.zarr/'.format(year=dtime.year, month=dtime.month,var=var)\n","    return xr.open_zarr(s3fs.S3Map(f_zarr, s3=fs))\n","\n","def gen_zarr_range(start, end,var):\n","    return [get_z(tt,var) for tt in gen_d_range(start, end)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ek1ZKUgozjf4"},"source":["tmp_a = gen_zarr_range(datetime.datetime(1980,1,1), datetime.datetime(1980,3,31),'air_temperature_at_2_metres')\n","tmp_all = xr.concat(tmp_a, dim='time0')\n","tmp_all = tmp_all.air_temperature_at_2_metres.chunk({'time0': -1}).resample(time0='1D').max()\n","tmp_all = tmp_all.quantile( [0.9,0.95], dim = 'time0' )\n","tmp_all.to_netcdf('/content/drive/My Drive/data/livestock/t2m_90_quantile.nc4')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6v3o95oC0ywv"},"source":["resample(time0='1D').quantile( [0.9,0.95], dim = 'time0' )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pFKi6wf_0BKK"},"source":["tmp_all"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o-d4x5ooG4aa"},"source":["# Climate data"]},{"cell_type":"code","metadata":{"id":"8iSvz4e1HSkp"},"source":["ds = xr.open_zarr( fsspec.get_mapper('gcs://pangeo-era5/reanalysis/temporal-analysis'), \n","                  consolidated = True, chunks = 'auto')# {'time': -1, 'latitude':10, 'longitude':10}  )\n","ds"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MWVac4kwOrzw"},"source":["path = '/content/drive/My Drive/data/livestock/t2m_90_quantile.nc4'\n","quants = [0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99]\n","ds.t2m.quantile( quants, dim = 'time' ).to_netcdf(path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oY-rVkJwj5C6"},"source":["path = '/content/drive/My Drive/data/livestock/1984_d2m_90up_quantile.nc4'\n","quants = [0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99]\n","with ProgressBar():\n","    ds.d2m.sel( time = slice('1984-01-01T00:00:00', '1984-01-01T23:00:00') ).to_netcdf(path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WKJqrzJyl50i"},"source":["d = ds.t2m.sel( time = '1984-01-01T00:00:00' ).values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K-80tZSTmzdL"},"source":["ds.t2m.sel(time='2000-12-31T23:00:00').squeeze().plot()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eFn-Ur_Wmhfr"},"source":["d = d.to_netcdf(path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9pmWziUaSV9W"},"source":["path = '/content/drive/My Drive/data/livestock/1984_d2m_90up_quantile.nc4'\n","quants = [0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99]\n","d = ds.t2m.sel(time = slice('1984-01-01T00:00:00', '1984-01-31T23:00:00')).quantile( quants, dim = 'latitude' )\n","\n","with ProgressBar():\n","    out = d.compute()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"27uJEp8eIgEw"},"source":["path = '/content/drive/My Drive/data/livestock/t2m_90_quantile.nc4'\n","quants = [0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99]\n","ds.t2m.sel( latitude = slice(90,87.5) ).resample( time = '1D' ).max().quantile( quants, dim = 'time' ).to_netcdf(path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"77vJONUqO4hQ"},"source":["### CMIP6"]},{"cell_type":"code","metadata":{"id":"9h4R0pu2GtRX"},"source":["df = pd.read_csv( 'https://storage.googleapis.com/cmip6/cmip6-zarr-consolidated-stores.csv' )\n","df = df.query( \"activity_id=='ScenarioMIP' & variable_id == 'tas' & experiment_id == 'ssp585' & source_id == 'IPSL-CM6A-LR' & table_id == 'day'\" )\n","df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2v1QKwDWGvjD"},"source":["ds = xr.open_zarr( gcs.get_mapper( df.zstore.values[0] ), consolidated = True )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4-QeFcAblmjA"},"source":["import time\n","import xarray as xr\n","import numpy as np\n","\n","from tqdm import tqdm_notebook\n","from xarray.core.groupby import DataArrayGroupBy, DatasetGroupBy\n","\n","def inner_generator(df_function='apply'):\n","    def inner(df, func, *args, **kwargs):\n","        \n","        t = tqdm_notebook(total = len(df))\n","        def wrapper(*args, **kwargs):\n","            t.update( n=1 if not t.total or t.n < t.total else 0 )\n","            return func( *args, **kwargs )\n","\n","        result = getattr(df, df_function)(wrapper, **kwargs)\n","\n","        t.close()\n","        return result\n","    return inner\n","\n","DataArrayGroupBy.progress_apply = inner_generator()\n","DatasetGroupBy.progress_apply = inner_generator()\n","\n","arr = xr.DataArray( np.arange( 10 ), dims = [ 'x' ] )\n","\n","def foo( v ):\n","    time.sleep( 1 )\n","    return v + 15\n","\n","print( arr.groupby( 'x' ).progress_apply( foo ) )"],"execution_count":null,"outputs":[]}]}